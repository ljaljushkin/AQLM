{'input_ids': tensor([[7434]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0'), 'position_ids': tensor([[0]], device='cuda:0')}
Tune:  model._nncf.external_quantizers.FQ_LORA_for_node_layers_23_mlp_down_proj_weight.input_low
trainable params: 262,144 || all params: 2,051,115,176 || trainable%: 0.0128
ilt=torch.bfloat16 il=0.0184326171875, ir=0.0196533203125 irt=torch.bfloat16
 tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], device='cuda:2', dtype=torch.bfloat16)
epoch 0         | total updates = 1     loss = 0.064632647
ilt=torch.bfloat16 il=0.036865234375, ir=0.0167236328125 irt=torch.bfloat16
 tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], device='cuda:2', dtype=torch.bfloat16)
epoch 1         | total updates = 2     loss = 0.062495865
ilt=torch.bfloat16 il=0.06103515625, ir=0.032470703125 irt=torch.bfloat16
 tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], device='cuda:2', dtype=torch.bfloat16)
epoch 2         | total updates = 3     loss = 0.062225387
GPU max memory allocated: 1.52 GB.
GPU max memory reserved: 1.55 GB.
eval: torch.cuda.max_memory_allocated()=1,561,230,336